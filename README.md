# Learnathon_BFSI

ğŸš— AI-Powered Fraud Detection in Auto Insurance
ğŸ§  Predictive Modeling for Smarter Claims Management

ğŸ“Œ Overview
The AI-Powered Fraud Detection in Auto Insurance project leverages machine learning (ML) to identify fraudulent claims in the auto insurance industry, addressing a critical issue that results in billions of dollars in losses annually. By automating fraud detection, this solution helps insurers reduce financial strain, lower premiums for customers, and streamline claims management. The system uses structured historical claim data to train a robust ML model, prioritizing high-risk claims for investigation while minimizing false positives to maintain customer trust.
Key objectives:

Automate fraud detection in real-time.
Prioritize claims for deeper investigation.
Enhance efficiency in the claims management pipeline.


ğŸ“Œ Problem Statement
Insurance fraud, including staged accidents, exaggerated injuries, and falsified claims, significantly impacts the auto insurance industry. Manual detection is time-consuming and error-prone. This project aims to develop an AI-driven fraud detection system that:

Identifies suspicious claims with high accuracy.
Optimizes resource allocation by flagging high-risk claims.
Reduces false positives to ensure fair treatment of customers.
Improves operational efficiency by automating the claims review process.

The solution uses historical claim data to train ML models, enabling proactive fraud detection and cost reduction.

ğŸ“Š Dataset Description
The dataset (auto_insurance.csv) contains structured claim records with over 50 features, including policyholder demographics, accident details, vehicle information, and financial data. A detailed schema is provided in data_dictionary.txt.
ğŸ“ Key Features

Claim_ID: Unique identifier for each claim (e.g., CLM12345).
Customer_Life_Value1: Estimated lifetime value of the customer.
Demographics:
Age_Insured: Age of the policyholder.
Gender: Male/Female/Other.
Education: Educational attainment (e.g., High School, Bachelorâ€™s).
Occupation: Job category (e.g., Professional, Blue-Collar).


Accident Details:
Accident_Date: Date of the accident.
Accident_Severity: Severity level (Minor, Moderate, Severe).
Accident_Type: Type of incident (Collision, Theft, Vandalism).
Accident_Hour: Time of day the accident occurred.


Vehicle Information:
Annual_Mileage: Estimated yearly mileage.
DiffIN_Mileage: Difference between reported and expected mileage.
Low_Mileage_Discount: Indicator of low mileage discount (Y/N).


Claim Financials:
Total_Claim: Total claimed amount.
Injury_Claim: Portion for bodily injuries.
Property_Claim: Portion for property damage.
Vehicle_Claim: Portion for vehicle damage.
Capital_Gains/Loss: Financial gains or losses.


Target Variable:
Fraud_Ind: Binary fraud indicator (Y = Fraudulent, N = Non-Fraudulent).



ğŸ“ˆ Data Characteristics

Size: 100,000+ records (varies by dataset).
Imbalance: Fraudulent claims are rare (5-10%), requiring SMOTE for balancing.
Challenges: Missing values, categorical features, and outliers in financial data.

For a complete schema, see data_dictionary.txt.

âš™ï¸ Features of the Solution
The fraud detection system includes a comprehensive pipeline:

Duplicate Detection & Removal: Eliminates duplicate Claim_ID entries.
Missing Value Handling: Imputes missing data using mode (categorical), mean/median (numerical), or forward fill (temporal).
Outlier Detection: Uses Z-Score and IQR methods to identify and handle outliers in financial and mileage features.
Feature Encoding & Scaling: Applies Label/One-Hot Encoding for categorical features and StandardScaler for numerical features.
Data Imbalance Handling: Uses SMOTE to balance the minority class (Fraud_Ind = Y).
Model Training: Evaluates 10 ML classifiers with 5-fold cross-validation.
Evaluation Metrics: Measures accuracy, precision, recall, F1-score, and ROC-AUC, prioritizing recall and F1-score.
Model Comparison: Visualizes performance with bar plots and ROC curves.
Fraud Prediction: Applies the best model (Random Forest + SMOTE) to unseen claims.


ğŸ“‚ Project Structure
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ auto_insurance.csv          # Raw dataset
â”‚   â””â”€â”€ data_dictionary.txt         # Feature schema
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ exploratory_analysis.ipynb  # EDA notebook
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py              # Data cleaning and preprocessing
â”‚   â”œâ”€â”€ model.py                   # Model training and prediction
â”‚   â””â”€â”€ evaluate.py                # Model evaluation and visualization
â”œâ”€â”€ models/
â”‚   â””â”€â”€ saved_model.pkl            # Trained Random Forest model
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ requirements.txt               # Python dependencies

ğŸ“‚ File Descriptions

data/: Raw dataset and feature schema.
notebooks/exploratory_analysis.ipynb: Jupyter notebook for exploratory data analysis (EDA).
src/preprocess.py: Handles data cleaning, imputation, encoding, scaling, and SMOTE.
src/model.py: Trains and saves ML models.
src/evaluate.py: Evaluates models and generates visualizations.
models/saved_model.pkl: Serialized Random Forest model.
requirements.txt: Lists Python dependencies.


ğŸ§ª Algorithms Used
Ten ML classifiers were evaluated (metrics are illustrative):



Model
Accuracy
Precision
Recall
ROC-AUC



Logistic Regression
0.85
0.80
0.75
0.88


Decision Tree
0.82
0.78
0.80
0.85


Random Forest
0.90
0.88
0.85
0.92


Gradient Boosting
0.89
0.86
0.84
0.91


AdaBoost
0.87
0.83
0.82
0.89


KNN
0.83
0.79
0.78
0.86


SVM
0.86
0.82
0.80
0.88


Naive Bayes
0.80
0.75
0.77
0.84


Extra Trees
0.88
0.85
0.83
0.90


Bagging Classifier
0.87
0.84
0.82
0.89


ğŸ† Best Model
Random Forest with SMOTE was selected for:

High F1-Score (~86%): Balances precision and recall.
High Recall (~85%): Captures most fraudulent claims.
Low False Positives: Minimizes unnecessary investigations.
Robustness: Handles complex feature interactions.


ğŸ§  How It Works (Pipeline Overview)
The pipeline is implemented in Python:

Load Dataset: Reads auto_insurance.csv using pandas.
Drop Duplicates: Removes duplicate Claim_ID entries.
Handle Missing Values: Imputes using mode, mean, or forward fill.
Encode Categorical Features: Uses LabelEncoder for ordinal and OneHotEncoder for nominal features.
Outlier Detection: Applies Z-Score to filter outliers (e.g., in Total_Claim).
Feature Scaling: Normalizes numerical features with StandardScaler.
Train/Test Split: Splits data (80/20) with random_state=42.
Handle Imbalance: Applies SMOTE to balance training data.
Train Models: Trains 10 classifiers with cross-validation.
Evaluate Metrics: Computes accuracy, precision, recall, F1, and ROC-AUC.
Save Model: Serializes the best model (Random Forest) to saved_model.pkl.

Example code snippet (from model.py):
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from imblearn.over_sampling import SMOTE
from scipy.stats import zscore
import joblib

# Load dataset
data = pd.read_csv("data/auto_insurance.csv")

# Drop duplicates
data = data.drop_duplicates(subset="Claim_ID")

# Handle missing values
data["Annual_Mileage"].fillna(data["Annual_Mileage"].mean(), inplace=True)
data["Gender"].fillna(data["Gender"].mode()[0], inplace=True)

# Encode categorical features
data["Accident_Severity"] = LabelEncoder().fit_transform(data["Accident_Severity"])
data = pd.get_dummies(data, columns=["Gender", "Accident_Type"])

# Outlier detection
data = data[abs(zscore(data["Total_Claim"])) < 3]

# Feature scaling
scaler = StandardScaler()
numerical_cols = ["Age_Insured", "Annual_Mileage", "Total_Claim"]
data[numerical_cols] = scaler.fit_transform(data[numerical_cols])

# Train/test split
X = data.drop("Fraud_Ind", axis=1)
y = data["Fraud_Ind"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle imbalance
smote = SMOTE(random_state=42)
X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)

# Train model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train_bal, y_train_bal)

# Save model
joblib.dump(model, "models/saved_model.pkl")


ğŸ“Œ Results & Insights
ğŸ“ˆ Model Performance

Random Forest:
Accuracy: ~90%.
Precision: ~88%.
Recall: ~85%.
F1-Score: ~86%.
ROC-AUC: ~92%.


SMOTE improved recall by ~15%.
False Positives: Kept low to avoid unnecessary investigations.

ğŸ§  Key Insights

Feature Importance:
DiffIN_Mileage: Large discrepancies often indicate fraud.
Low_Mileage_Discount: Claims with discounts are less likely to be fraudulent.
Accident_Hour: Late-night/early-morning accidents correlate with fraud.
Capital_Gains/Loss: Unusual financial patterns are strong fraud indicators.


EDA Findings:
Fraud is more common in Accident_Type like staged collisions.
Younger policyholders (Age_Insured < 30) show slightly higher fraud rates.
Financial features (Total_Claim, Injury_Claim) strongly correlate with fraud.



ğŸ“Š Visualization

Bar Plot: Compares model performance metrics.
ROC Curve: Shows Random Forestâ€™s superior discrimination.
Feature Importance Plot: Highlights key predictors.


ğŸ›  Installation & Usage
ğŸ“¦ Prerequisites

Python 3.8+
Git
Jupyter Notebook

ğŸ›  Setup

Clone the repository:
git clone https://github.com/abhisek2004/auto-insurance-fraud-detection.git
cd auto-insurance-fraud-detection


Install dependencies:
pip install -r requirements.txt


Run the pipeline:
python src/model.py


Explore data:
jupyter notebook notebooks/exploratory_analysis.ipynb



ğŸ“¦ Requirements
pandas==2.0.3
numpy==1.24.3
matplotlib==3.7.2
seaborn==0.12.2
scikit-learn==1.3.0
imbalanced-learn==0.11.0
xgboost==2.0.0


ğŸ“Œ Future Enhancements

Streamlit Dashboard: Deploy an interactive UI for real-time fraud prediction and visualization.
Deep Learning: Use LSTM or AutoEncoders for sequential fraud patterns.
Time-Series Analysis: Visualize fraud trends over time.
Real-Time Integration: Connect to claim submission systems via xAIâ€™s API (https://x.ai/api).
Feature Expansion: Incorporate external data (e.g., weather, traffic).
Explainability: Use SHAP or LIME for interpretable predictions.


ğŸ“œ License
This project is licensed under the MIT License.

ğŸ‘¨â€ğŸ’» Developed By

Abhisek Panda â€“ GitHub
Team: Learnathon_BFSI


ğŸ“Œ Contributing
Contributions are welcome! Please submit a pull request or open an issue on GitHub.

ğŸ“Œ Contact
For questions or feedback, contact Abhisek Panda or explore xAIâ€™s API for integration: https://x.ai/api.
